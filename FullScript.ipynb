{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-10T09:58:47.289573200Z",
     "start_time": "2025-06-10T09:58:42.602280Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train, validation and test set creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Loading the spectrograms for input\n",
    "def load_X():\n",
    "  with h5py.File('X_Slowed_Sped15.mat','r') as f:       # Open the .mat datafile\n",
    "      cell_ds = f['outputSpectrograms']                 # Select the spectrograms\n",
    "      refs = cell_ds.flatten[0, :]                      # Remove empty dimension (sometimes [:,0] is needed when dimensions are swapped\n",
    "      all_specs = np.array([f[r][()] for r in refs])    # Make a numpy array of all spectrograms\n",
    "      all_specs = all_specs.transpose((0, 3, 2, 1))     # Matlab uses a different order of dimensions, so reshape it so the dimensions equal to the dimensions in matlab\n",
    "      X = torch.tensor(all_specs, dtype=torch.float32)  # Create a tensor\n",
    "  return X\n",
    "\n",
    "# Loading the labels\n",
    "def load_y():\n",
    "  activities = []\n",
    "  with h5py.File('y_Slowed_Sped15.mat', 'r') as f:\n",
    "      cell_ds = f['fileLabels']\n",
    "      # Loop over every sample and append the activity to the list\n",
    "      for i in range(len(cell_ds)):\n",
    "          struct_ref = cell_ds[i, 0]\n",
    "          struct_grp = f[struct_ref]\n",
    "          field_obj = struct_grp['activity']\n",
    "          activity = field_obj[0, 0]\n",
    "          activities.append(activity)\n",
    "  return torch.tensor(activities, dtype=torch.long)\n",
    "\n",
    "def get_dataloaders():\n",
    "  X = load_X().unsqueeze(1) # Unsqueeze the second dimension because a CNN expects a [n, 1, H, W] shape, where H and W are the height and width of the spectrogram\n",
    "  y = load_y()\n",
    "  return X, y\n",
    "\n",
    "# Load the pytorch arrays and save them in a numpy array\n",
    "X, y = get_dataloaders()\n",
    "np.save('X.npy', X)\n",
    "np.save('y.npy', y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T09:34:50.880986300Z",
     "start_time": "2025-06-10T09:34:50.873831400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the train, validation and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def make_loaders(batch_size,\n",
    "                 train_frac=0.7, val_frac=0.15,\n",
    "                 num_workers=4, seed=1):\n",
    "    X = np.load('X_Slowed_Sped15.npy')\n",
    "    y = np.load('y_Slowed_Sped15.npy')-1\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "    N = X.size(0)\n",
    "\n",
    "    # Divide samples between the train, test and validation set randomly\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    perm = torch.randperm(N, generator=g)   # Creating a random permutation of the indices\n",
    "    n_train = int(train_frac * N)\n",
    "    n_val   = int(val_frac   * N)\n",
    "    train_idx = perm[:n_train]              # Selecting indices for the train, val and test set\n",
    "    val_idx   = perm[n_train:n_train+n_val]\n",
    "    test_idx  = perm[n_train+n_val:]\n",
    "\n",
    "    # build train set\n",
    "    X_tr = X[train_idx]                      # (n_train,1,300,347,3)\n",
    "    y_tr = y[train_idx]                      # (n_train,)\n",
    "\n",
    "    # We have for every sample 3 versions, the normal, slowed and sped version. In the train set we use all, in the test and validation set we only use the normal version.\n",
    "\n",
    "    # bring the aug‐axis next to batch, then flatten\n",
    "    X_tr = X_tr.permute(0,4,1,2,3)           # (n_train,3,1,300,347)\n",
    "    Naug, C, H, W = 3, X_tr.size(2), X_tr.size(3), X_tr.size(4)\n",
    "    X_tr = X_tr.reshape(-1, C, H, W)         # (n_train*3,1,300,347)\n",
    "    y_tr = y_tr.repeat_interleave(Naug)      # (n_train*3,)\n",
    "\n",
    "    # Make class weights\n",
    "    counts = np.bincount(y, minlength=6)  # length=6\n",
    "    class_wts = torch.tensor([1.0/count for count in counts], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create the train loader\n",
    "    train_ds = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # build val set (only first aug: index 0)\n",
    "    X_val = X[val_idx, :, :, :, 0]           # (n_val,1,300,347)\n",
    "    y_val = y[val_idx]\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # build test set (only first aug)\n",
    "    X_test = X[test_idx, :, :, :, 0]\n",
    "    y_test = y[test_idx]\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_wts\n",
    "\n",
    "# --- Creating the Loaders ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, val_loader, test_loader, class_wts = make_loaders(batch_size=32, train_frac=0.7, val_frac=0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T09:58:56.880334800Z",
     "start_time": "2025-06-10T09:58:51.913384600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class RadarCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    3-conv + 3-pool + 3-dense CNN for 600×693 Doppler spectrograms.\n",
    "      Layer-1 : Conv2d( 1 → 50 , 2×2) + ReLU\n",
    "      Layer-2 : MaxPool2d(3)\n",
    "      Layer-3 : Conv2d(50 → 50 , 2×2) + ReLU\n",
    "      Layer-4 : MaxPool2d(3)\n",
    "      Layer-5 : Conv2d(50 → 50 , 2×2) + ReLU\n",
    "      Layer-6 : MaxPool2d(3)\n",
    "      Layer-7-8 : two dense layers\n",
    "      Output  : Soft-max over 6 classes\n",
    "    Xavier-uniform initialisation is applied to *all* conv and dense layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conv_channels=50, kernel_size=2, pool_size=3, p_conv=0.0, n_classes: int = 6, img_size=(600, 693)):\n",
    "        super().__init__()\n",
    "\n",
    "        # ─── Convolution / Pooling trunk ─────────────────────────────────────────\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, conv_channels, kernel_size=kernel_size, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=p_conv),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "            # Block 2\n",
    "            nn.Conv2d(conv_channels, conv_channels, kernel_size=kernel_size, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=p_conv),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "            # Block 3\n",
    "            nn.Conv2d(conv_channels, conv_channels, kernel_size=kernel_size, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=p_conv),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "        )\n",
    "\n",
    "\n",
    "        # ─── Work out the flattened size automatically ──────────────────────────\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, *img_size)          # BCHW = (1,1,600,693)\n",
    "            print(self.features(dummy).shape)\n",
    "            n_flat = self.features(dummy).numel()\n",
    "\n",
    "        # ─── Three dense layers + soft-max ──────────────────────────────────────\n",
    "        self.classifier = nn.Sequential(                  # Layers 7–8\n",
    "            nn.Linear(n_flat, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),                              # Dropout layer to prevent overfitting\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "        self._init_weights()                              # Xavier-uniform\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)          # (B,50,H’,W’)\n",
    "        x = torch.flatten(x, 1)       # -> (B, n_flat)\n",
    "        x = self.classifier(x)        # -> (B, n_classes)\n",
    "        return x                      # (logits; use softmax or CE-loss outside)\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier-uniform on every conv and linear weight; zero bias.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T10:04:16.013453Z",
     "start_time": "2025-06-10T10:04:16.001608800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, device, num_epochs, patience):\n",
    "    \"\"\"\n",
    "    Trains and validates the model, returning the best model (highest validation accuracy).\n",
    "\n",
    "    Args:\n",
    "        model       : nn.Module, the neural network to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader  : DataLoader for validation data\n",
    "        criterion   : loss function, e.g., nn.CrossEntropyLoss()\n",
    "        optimizer   : optimizer, e.g., torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        device      : 'cuda' or 'cpu'\n",
    "        num_epochs  : number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        best_model  : state_dict of the model with the highest validation accuracy\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve= 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Take a gradient descent step\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Values to later calculate Train Loss\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            running_total += labels.size(0)\n",
    "\n",
    "        # Calculate train loss\n",
    "        epoch_loss = running_loss / running_total\n",
    "        epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # Calculate Validation Loss\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_epoch_loss = val_loss / val_total\n",
    "        val_epoch_acc = val_corrects.double() / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | '\n",
    "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # deep copy the best model if validation acc has improved\n",
    "        if val_epoch_acc > best_acc:\n",
    "            best_acc = val_epoch_acc\n",
    "            best_model_wts = deepcopy(model.state_dict())\n",
    "            epochs_no_improve= 0\n",
    "        # If 5 epochs do not improve validation acc further, then stop training\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} '\n",
    "                      f'after {patience} epochs with no val_loss improvement.')\n",
    "                break\n",
    "\n",
    "    # Calculate test loss\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            test_corrects += torch.sum(preds == labels.data)\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_epoch_acc = test_corrects.double() / test_total\n",
    "    print(test_epoch_acc)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model, \"full_model.pth\")\n",
    "    return model\n",
    "\n",
    "# Usage example:\n",
    "im_size = train_loader.dataset[0][0][0].shape\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")      # Train on GPU\n",
    "model     = RadarCNN(img_size=(im_size[0], im_size[1])).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wts)                          # Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)                  # Adam Optimizer\n",
    "best_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, device, num_epochs=32, patience=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the Model and apply on the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create needed variables\n",
    "im_size = train_loader.dataset[0][0][0].shape\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")      # Train on GPU\n",
    "model     = RadarCNN(img_size=(im_size[0], im_size[1])).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wts)                          # Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)                  # Adam Optimizer\n",
    "\n",
    "# Calculate test loss\n",
    "model = torch.load(\"full_model.pth\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_corrects = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        test_corrects += torch.sum(preds == labels.data)\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_epoch_loss = test_loss / test_total\n",
    "test_epoch_acc = test_corrects.double() / test_total\n",
    "print(test_epoch_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
